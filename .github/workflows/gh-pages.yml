name: GitHub pages and scraping

on:
  push:
    branches:
      - main
  schedule:
    - cron: "0 12 * * *" # every day at 12:00PM

jobs:
  build-deploy:
    runs-on: ubuntu-18.04
    steps:
    - uses: actions/checkout@master
    - name: Cache dependencies
      uses: actions/cache@v1
      with:
        path: ~/.npm
        key: ${{ runner.os }}-node-${{ hashFiles('**/yarn.lock') }}
        restore-keys: |
          ${{ runner.os }}-node-
    - run: git clone https://github.com/meilisearch/specifications
    - run: yarn install
    - run: yarn build
    - run: rm -rf docs
    - run: cp -r .vuepress/dist docs
    - run: |
        git config --global user.name "tamo Le bot"
        git config --global user.email "tamo@meilisearch.com"
        
        git add docs
        git commit -m "update the website according to the specs"
        git push

  run-scraper:
    needs: build-deploy
    runs-on: ubuntu-18.04
    steps:
      - uses: actions/checkout@master
      - name: Run scraper
        env:
          HOST_URL: ${{ secrets.MEILISEARCH_HOST_URL }}
          API_KEY: ${{ secrets.MEILISEARCH_API_KEY }}
          CONFIG_FILE_PATH: ${{ github.workspace }}/.vuepress/scraper/
        run: |
          docker run -t --rm \
            -e MEILISEARCH_HOST_URL=$HOST_URL \
            -e MEILISEARCH_API_KEY=$API_KEY \
            -v $CONFIG_FILE_PATH:/config \
            getmeili/docs-scraper:v0.12.1 pipenv run ./docs_scraper /config/config.json
